# í™˜ê²½ ì„¤ì •
import os
os.environ['HF_CODE'] = "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"

from huggingface_hub import login
login(token=os.environ.get("HF_CODE"))

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
import torch
from transformers import (
    BitsAndBytesConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    pipeline,
    logging
)
from peft import PeftModel

def model_generator(base_model, adapter_dir):
    compute_dtype = getattr(torch, "float16")
    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=False,
    )

    device_map = {"": 0} if torch.cuda.is_available() else {"": -1}
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        quantization_config=quant_config,
        device_map=device_map
    )
    model.config.use_cache = False
    model.config.pretraining_tp = 1
    model.eval()

    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # ì–´ëŒ‘í„° ì¶”ê°€
    model = PeftModel.from_pretrained(model, adapter_dir)
    return model

base_model = "beomi/llama-2-ko-7b"
adapter_dir = "./weights/AHT_5"
model = model_generator(base_model, adapter_dir)

# ì±—ë´‡
import re
import streamlit as st

def QA_generator(prompt):
    query = f"<s>[INST] {prompt} [/INST]"

    pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=100)
    result = pipe(query)
    response = result[0]['generated_text']
    if "ì¶”ì²œí•©ë‹ˆë‹¤." in response:
        pattern = r'\[/INST\] (.*?) ì¶”ì²œí•©ë‹ˆë‹¤\.'
        match = re.search(pattern, response)
        if match:
            response = match.group(0)[8:]
    elif "ì•Œë ¤ë“œë¦´ê²Œìš”." in response:
        pattern = r'\[/INST\] (.*?) ì•Œë ¤ë“œë¦´ê²Œìš”\.'
        match = re.search(pattern, response, re.DOTALL)
        if match:
            response = match.group(0)[8:]

    print(f"Q. {prompt}")
    print(f"A. {response}")

logging.set_verbosity(logging.CRITICAL)
st.title("AI í—¬ì‹œí†¡ğŸ’¬")
st.caption("ì§ˆí™˜ ì˜ˆì¸¡ ë° ì§„ë£Œê³¼ ì¶”ì²œì„ ìœ„í•œ AI ì–´í”Œë¦¬ì¼€ì´ì…˜") # ğŸš€

chat_history = []

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "í˜„ì¬ ê²ªê³  ê³„ì‹œëŠ” ì¦ìƒì„ ë§ì”€í•´ì£¼ì„¸ìš”."}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    response = get_llama_response(st.session_state.messages[-1]['content'], chat_history)
    msg = response
    st.session_state.messages.append({"role": "assistant", "content": msg})
    st.chat_message("assistant").write(msg)