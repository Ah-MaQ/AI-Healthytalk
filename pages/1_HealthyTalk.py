import torch
from transformers import (
    BitsAndBytesConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    pipeline
)
from peft import PeftModel
from collections import deque
import re
import streamlit as st
from streamlit_cookies_manager import EncryptedCookieManager
import streamlit.components.v1 as components

st.set_page_config(
    page_title="AI í—¬ì‹œí†¡",
    page_icon="ğŸ’¬",
    layout="wide"
)

cookies = EncryptedCookieManager(prefix="my_project", password="my_super_secret_password")
if not cookies.ready():
    st.stop()

@st.cache_resource
def model_generator(base_model, adapter_dir):
    compute_dtype = getattr(torch, "float16")
    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=False,
    )


    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        quantization_config=quant_config,
        device_map={"": 0}
    )
    model.config.use_cache = False
    model.config.pretraining_tp = 1

    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # ì–´ëŒ‘í„° ì¶”ê°€
    model = PeftModel.from_pretrained(model, adapter_dir)
    pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

    return pipe


def getHistroy():
    history = ''
    for c in st.session_state.messages:
        r, m = c.values()
        if r == 'user':
            history += m + ' '
    return history

def QA_generator(query):
    prompt = f"<s>[INST] {query} [/INST]"

    result = pipe(prompt)
    try:
        response = result[0]['generated_text'].split("[/INST] ")[1]
        if "ì¶”ì²œí•©ë‹ˆë‹¤." in response:
            pattern = r'(.*?) ì¶”ì²œí•©ë‹ˆë‹¤\. '
            match = re.search(pattern, response)
            if match:
                response = match.group(0)
                symptom, answer = response.split(" [SYM]")
            else:
                answer = response

        elif "ì•Œë ¤ë“œë¦´ê²Œìš”." in response:
            pattern = r'(.*?) ì•Œë ¤ë“œë¦´ê²Œìš”\. '
            match = re.search(pattern, response, re.DOTALL)
            if match:
                answer = match.group(0)
            else:
                answer = response
        else:
            answer = result[0]['generated_text']
    except:
        answer = result[0]['generated_text']

    print(f"Q. {query}")
    print(f"A. {answer}")

    return response

# ================================

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    if cookies.get('name'):
        st.write(f"{cookies['name']}ë‹˜ì´ ë¡œê·¸ì¸ ì¤‘ì…ë‹ˆë‹¤.")
    if cookies.get('name'):
        if st.button('ë¡œê·¸ì•„ì›ƒ'):
            cookies['username'] = ""
            cookies['name'] = ""
            cookies.save()  # ì¿ í‚¤ ë³€ê²½ ì‚¬í•­ ì €ì¥
            if "messages" in st.session_state:
                del st.session_state["messages"]
            if "location" in st.session_state:
                del st.session_state["location"]
            st.session_state['page'] = 'healthytalk'
            st.session_state['page'] = 'nearby_hostpital'
            st.session_state['page'] = 'login'  # ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™
            components.html("""
                <script>
                    window.location.reload();
                </script>
            """)
            st.experimental_rerun()

st.header("AI í—¬ì‹œí†¡ğŸ’¬")
st.subheader("ì§ˆí™˜ ì˜ˆì¸¡ ë° ì§„ë£Œê³¼ ì¶”ì²œì„ ìœ„í•œ AI ì–´í”Œë¦¬ì¼€ì´ì…˜") # ğŸš€

chat_col, detail_col = st.columns([2, 3])


base_model = "beomi/llama-2-ko-7b"
adapter_dir = "./weights/AHT_8"
pipe = model_generator(base_model, adapter_dir)

css = """
<style>
    .custom-column {
        height: 400px;
    }
</style>
"""
st.markdown(css, unsafe_allow_html=True)

# padding: 10
# px;
# margin: 10
# px;
# border - radius: 10
# px;
with chat_col:
    chat_col.markdown('<div class="custom-column"></div>', unsafe_allow_html=True)
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant",
                                         "content": "ì €ëŠ” ì‚¬ìš©ìì˜ ë¶ˆí¸ì„ ìµœì†Œí™” í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì˜ë£Œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ì…ë‹ˆë‹¤. ğŸ©º \n" + \
                                                    "í˜„ì¬ ê²ªê³  ê³„ì‹œëŠ” ì¦ìƒì´ ìˆë‹¤ë©´ ê°€ê°ì—†ì´ ë§í•´ì£¼ì„¸ìš”. ğŸ˜ƒ" + \
                                                    "ì˜ˆìƒë˜ëŠ” ì§ˆí™˜ê³¼ ì¸ê·¼ì˜ ë³‘ì›ì„ ì•Œë ¤ë“œë¦´ê²Œìš”."}]

    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    if prompt := st.chat_input():
        if not cookies.get('username'):
            st.error("ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            st.session_state.messages.append({"role": "user", "content": prompt})
            st.chat_message("user").write(prompt)
            print("###prompt###: ", prompt)
            # response = get_llama_response(st.session_state.messages[-1]['content'], chat_history)
            response = QA_generator(st.session_state.messages[-1]['content'])
            msg = response
            print("###QA result###: ", response)
            st.session_state.messages.append({"role": "assistant", "content": msg})
            st.chat_message("assistant").write(msg)
            print("###this is state", st.session_state)
            # print("###this is messages", st.session_state.messages)
            # print("###this is msg from user", st.session_state.messages["user"])

with detail_col:
    components.iframe("https://www.amc.seoul.kr/asan/healthinfo/disease/diseaseDetail.do?contentId=31969", scrolling=True, height=600)